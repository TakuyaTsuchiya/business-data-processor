"""
ã‚¢ãƒ¼ã‚¯æ®‹å‚µæ›´æ–°å‡¦ç†
ark-late-payment-updateãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®çµ±åˆç‰ˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
"""

import pandas as pd
import streamlit as st
import chardet
import io
import os
from datetime import datetime
from typing import Tuple, Optional, Dict, Any


def detect_encoding(file_content: bytes) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º"""
    result = chardet.detect(file_content[:10000])
    detected_encoding = result['encoding']
    confidence = result['confidence']
    
    # Streamlitã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿UIè¡¨ç¤º
    try:
        # st._get_script_run_ctx()ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­˜åœ¨ç¢ºèª
        if hasattr(st, '_get_script_run_ctx') and st._get_script_run_ctx() is not None:
            st.info(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º: {detected_encoding} (ä¿¡é ¼åº¦: {confidence:.2f})")
    except:
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã— ã¾ãŸã¯ ã‚¨ãƒ©ãƒ¼æ™‚ã¯é™ã‹ã«å‡¦ç†ç¶šè¡Œ
        pass
    
    # ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã¯æ—¥æœ¬èªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
    if confidence < 0.7:
        for encoding in ['cp932', 'shift_jis', 'utf-8']:
            try:
                file_content.decode(encoding)
                return encoding
            except UnicodeDecodeError:
                continue
    
    return detected_encoding or 'cp932'


def normalize_key_column(df: pd.DataFrame, column_name: str) -> pd.DataFrame:
    """ã‚­ãƒ¼é …ç›®ã®æ­£è¦åŒ–ï¼ˆç©ºç™½é™¤å»ã€å‹çµ±ä¸€ï¼‰"""
    if column_name not in df.columns:
        raise KeyError(f"ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {column_name}")
    
    df_normalized = df.copy()
    
    # æ–‡å­—åˆ—å¤‰æ›
    df_normalized[column_name] = df_normalized[column_name].astype(str)
    
    # ç©ºç™½é™¤å»ï¼ˆå…¨è§’ãƒ»åŠè§’ï¼‰
    df_normalized[column_name] = df_normalized[column_name].str.strip()
    df_normalized[column_name] = df_normalized[column_name].str.replace('ã€€', '', regex=False)
    
    # NaNã€ç©ºæ–‡å­—ã€'nan'ã®å‡¦ç†
    df_normalized = df_normalized[
        (df_normalized[column_name] != '') & 
        (df_normalized[column_name] != 'nan') &
        (df_normalized[column_name].notna())
    ]
    
    return df_normalized


def process_ark_late_payment_data(arc_file, contract_file) -> Optional[Tuple[pd.DataFrame, str]]:
    """
    ã‚¢ãƒ¼ã‚¯æ®‹å‚µæ›´æ–°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    
    Args:
        arc_file: ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆStreamlit UploadedFileï¼‰
        contract_file: ContractListãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆStreamlit UploadedFileï¼‰
        
    Returns:
        Tuple[pd.DataFrame, str]: (å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ , å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å)
    """
    try:
        # Phase 1: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        st.info("ğŸ“‚ Phase 1: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿")
        
        # ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVèª­ã¿è¾¼ã¿ï¼ˆbyteså½¢å¼å¯¾å¿œï¼‰
        try:
            arc_content = arc_file.getvalue()
            arc_encoding = detect_encoding(arc_content)
            arc_df = pd.read_csv(io.BytesIO(arc_content), encoding=arc_encoding)
            st.success(f"âœ… ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVèª­ã¿è¾¼ã¿å®Œäº†: {len(arc_df):,}è¡Œ")
        except Exception as e:
            st.error(f"âŒ ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            return None
        
        # ContractListèª­ã¿è¾¼ã¿ï¼ˆbyteså½¢å¼å¯¾å¿œï¼‰
        try:
            contract_content = contract_file.getvalue()
            contract_encoding = detect_encoding(contract_content)
            contract_df = pd.read_csv(io.BytesIO(contract_content), encoding=contract_encoding)
            st.success(f"âœ… ContractListèª­ã¿è¾¼ã¿å®Œäº†: {len(contract_df):,}è¡Œ")
        except Exception as e:
            st.error(f"âŒ ContractListãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            return None
        
        # Phase 2: ã‚«ãƒ©ãƒ ç¢ºèª
        st.info("ğŸ” Phase 2: å¿…é ˆã‚«ãƒ©ãƒ ç¢ºèª")
        
        # ã‚¢ãƒ¼ã‚¯æ®‹å‚µã®å¿…é ˆã‚«ãƒ©ãƒ 
        ark_required = {
            'contract_number': 'å¥‘ç´„ç•ªå·',
            'amount': 'ç®¡ç†å‰æ»ç´é¡'
        }
        
        # ContractListã®å¿…é ˆã‚«ãƒ©ãƒ 
        contract_required = {
            'takeover_number': 'å¼•ç¶™ç•ªå·',
            'management_number': 'ç®¡ç†ç•ªå·'
        }
        
        # ã‚«ãƒ©ãƒ å­˜åœ¨ç¢ºèª
        missing_ark = [col for col in ark_required.values() if col not in arc_df.columns]
        missing_contract = [col for col in contract_required.values() if col not in contract_df.columns]
        
        if missing_ark:
            st.error(f"âŒ ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_ark}")
            st.error(f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {list(arc_df.columns)}")
            return None
            
        if missing_contract:
            st.error(f"âŒ ContractListã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_contract}")
            st.error(f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {list(contract_df.columns)}")
            return None
        
        st.success("âœ… å¿…é ˆã‚«ãƒ©ãƒ ç¢ºèªå®Œäº†")
        
        # Phase 3: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ç´ä»˜ã‘
        st.info("ğŸ”— Phase 3: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ç´ä»˜ã‘")
        
        # ã‚­ãƒ¼é …ç›®ã®æ­£è¦åŒ–
        arc_df = normalize_key_column(arc_df, 'å¥‘ç´„ç•ªå·')
        contract_df = normalize_key_column(contract_df, 'å¼•ç¶™ç•ªå·')
        
        # ãƒ‡ãƒ¼ã‚¿çµåˆï¼ˆå¥‘ç´„ç•ªå·ã¨å¼•ç¶™ç•ªå·ã§ç´ä»˜ã‘ï¼‰
        merged_df = pd.merge(
            arc_df[['å¥‘ç´„ç•ªå·', 'ç®¡ç†å‰æ»ç´é¡']],
            contract_df[['å¼•ç¶™ç•ªå·', 'ç®¡ç†ç•ªå·']],
            left_on='å¥‘ç´„ç•ªå·',
            right_on='å¼•ç¶™ç•ªå·',
            how='inner'
        )
        
        # ç´ä»˜ã‘çµ±è¨ˆ
        matched_count = len(merged_df)
        arc_total = len(arc_df)
        unmatch_count = arc_total - matched_count
        match_ratio = (matched_count / arc_total * 100) if arc_total > 0 else 0
        
        st.write(f"ğŸ“Š ç´ä»˜ã‘çµæœ:")
        st.write(f"- ã‚¢ãƒ¼ã‚¯æ®‹å‚µç·æ•°: {arc_total:,}ä»¶")
        st.write(f"- ç´ä»˜ã‘æˆåŠŸ: {matched_count:,}ä»¶ ({match_ratio:.1f}%)")
        st.write(f"- ç´ä»˜ã‘å¤±æ•—: {unmatch_count:,}ä»¶")
        
        if matched_count == 0:
            st.error("âŒ ç´ä»˜ã‘å‡¦ç†ã®çµæœã€å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã«ãªã‚Šã¾ã—ãŸ")
            st.error("åŸå› : å¥‘ç´„ç•ªå·ã¨å¼•ç¶™ç•ªå·ã®å€¤ãŒä¸€è‡´ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            return None
        
        # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆç®¡ç†ç•ªå·ã€ç®¡ç†å‰æ»ç´é¡ã®2åˆ—ã®ã¿ï¼‰
        output_df = merged_df[['ç®¡ç†ç•ªå·', 'ç®¡ç†å‰æ»ç´é¡']].copy()
        
        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        output_df['ç®¡ç†ç•ªå·'] = output_df['ç®¡ç†ç•ªå·'].astype(str)
        output_df['ç®¡ç†å‰æ»ç´é¡'] = pd.to_numeric(output_df['ç®¡ç†å‰æ»ç´é¡'], errors='coerce').fillna(0).astype(int)
        
        # é‡è¤‡é™¤å»ï¼ˆç®¡ç†ç•ªå·ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€ç®¡ç†å‰æ»ç´é¡ã¯åˆè¨ˆï¼‰
        output_df = output_df.groupby('ç®¡ç†ç•ªå·', as_index=False).agg({
            'ç®¡ç†å‰æ»ç´é¡': 'sum'
        })
        
        # ã‚½ãƒ¼ãƒˆ
        output_df = output_df.sort_values('ç®¡ç†ç•ªå·')
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        current_date = datetime.now()
        output_filename = f"{current_date.strftime('%m%d')}ã‚¢ãƒ¼ã‚¯æ®‹å‚µ.csv"
        
        # å‡¦ç†ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        st.success("âœ… Phase 4: å‡¦ç†å®Œäº†")
        st.write(f"ğŸ“Š æœ€çµ‚å‡ºåŠ›:")
        st.write(f"- ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(output_df):,}ä»¶")
        st.write(f"- ç®¡ç†å‰æ»ç´é¡åˆè¨ˆ: Â¥{output_df['ç®¡ç†å‰æ»ç´é¡'].sum():,}")
        st.write(f"- å¹³å‡æ»ç´é¡: Â¥{output_df['ç®¡ç†å‰æ»ç´é¡'].mean():,.0f}")
        
        return output_df, output_filename
        
    except Exception as e:
        st.error(f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.error("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
        st.exception(e)
        return None
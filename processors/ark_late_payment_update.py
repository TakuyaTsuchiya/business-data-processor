"""
ã‚¢ãƒ¼ã‚¯æ®‹å‚µæ›´æ–°å‡¦ç†
ç®¡ç†ç•ªå·ã¨ç®¡ç†å‰æ»ç´é¡ã®2åˆ—ã®ã¿ã‚’å‡ºåŠ›
"""

import pandas as pd
import io
from datetime import datetime
from typing import Optional, Tuple, Union
import chardet

# Streamlitã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’æ¡ä»¶ä»˜ãã«ã™ã‚‹
try:
    import streamlit as st
    HAS_STREAMLIT = True
except ImportError:
    HAS_STREAMLIT = False

# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å€™è£œãƒªã‚¹ãƒˆï¼ˆGitHubã‚³ãƒ¼ãƒ‰ã‹ã‚‰è¸è¥²ï¼‰
ENCODING_CANDIDATES = ['cp932', 'shift_jis', 'utf-8-sig', 'utf-8']


def detect_encoding(file_content: Union[bytes, str]) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡ºã™ã‚‹"""
    if isinstance(file_content, str):
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ
        file_path = file_content
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)
    else:
        # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
        raw_data = file_content[:10000] if len(file_content) > 10000 else file_content
        file_path = None
    
    # chardetã§æ¤œå‡º
    result = chardet.detect(raw_data)
    detected_encoding = result['encoding']
    confidence = result.get('confidence', 0)
    
    if HAS_STREAMLIT:
        st.info(f"ğŸ“Š ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º: {detected_encoding} (ä¿¡é ¼åº¦: {confidence:.2f})")
    
    # ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã¯ä»£æ›¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
    if confidence < 0.7:
        if HAS_STREAMLIT:
            st.warning(f"âš ï¸ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¿¡é ¼åº¦ãŒä½ã„ãŸã‚ã€ä»£æ›¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã—ã¾ã™")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒã‚ã‚‹å ´åˆã®ã¿å®Ÿéš›ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆãŒå¯èƒ½
        if file_path:
            for encoding in ENCODING_CANDIDATES:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        f.read(1000)  # ãƒ†ã‚¹ãƒˆèª­ã¿è¾¼ã¿
                    if HAS_STREAMLIT:
                        st.success(f"âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° '{encoding}' ã§èª­ã¿è¾¼ã¿æˆåŠŸ")
                    return encoding
                except UnicodeDecodeError:
                    continue
        else:
            # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯å€™è£œã‹ã‚‰é¸æŠ
            for encoding in ENCODING_CANDIDATES:
                try:
                    raw_data.decode(encoding)
                    if HAS_STREAMLIT:
                        st.success(f"âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° '{encoding}' ã§ãƒ‡ã‚³ãƒ¼ãƒ‰æˆåŠŸ")
                    return encoding
                except UnicodeDecodeError:
                    continue
    
    # ã‚ˆãã‚ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ä¿®æ­£
    if detected_encoding in ['CP932', 'SHIFT_JIS', 'SHIFT-JIS']:
        return 'cp932'
    elif detected_encoding in ['UTF-8-SIG']:
        return 'utf-8-sig'
    elif detected_encoding:
        return detected_encoding
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if HAS_STREAMLIT:
            st.warning("âš ï¸ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã€‚cp932ã‚’ä½¿ç”¨ã—ã¾ã™")
        return 'cp932'


def normalize_key_column(df: pd.DataFrame, column_name: str) -> pd.DataFrame:
    """ã‚­ãƒ¼é …ç›®ã®æ­£è¦åŒ–ï¼ˆç©ºç™½é™¤å»ã€å‹çµ±ä¸€ï¼‰"""
    if column_name not in df.columns:
        raise KeyError(f"ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {column_name}")
    
    df_normalized = df.copy()
    
    # æ–‡å­—åˆ—å¤‰æ›
    df_normalized[column_name] = df_normalized[column_name].astype(str)
    
    # ç©ºç™½é™¤å»ï¼ˆå…¨è§’ãƒ»åŠè§’ï¼‰
    df_normalized[column_name] = df_normalized[column_name].str.strip()
    df_normalized[column_name] = df_normalized[column_name].str.replace('ã€€', '', regex=False)
    
    # NaNã€ç©ºæ–‡å­—ã€'nan'ã®å‡¦ç†
    df_normalized = df_normalized[
        (df_normalized[column_name] != '') & 
        (df_normalized[column_name] != 'nan') &
        (df_normalized[column_name].notna())
    ]
    
    return df_normalized


def process_ark_late_payment_data(arc_file, contract_file) -> Optional[Tuple[pd.DataFrame, str]]:
    """
    ã‚¢ãƒ¼ã‚¯æ®‹å‚µæ›´æ–°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    
    Args:
        arc_file: ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€UploadedFileã€ã¾ãŸã¯ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
        contract_file: ContractListãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€UploadedFileã€ã¾ãŸã¯ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
        
    Returns:
        Tuple[pd.DataFrame, str]: (å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ , å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å)
        ã‚¨ãƒ©ãƒ¼æ™‚ã¯None
    """
    try:
        # Phase 1: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        if HAS_STREAMLIT:
            st.info("ğŸ“‚ Phase 1: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿")
        
        # ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVèª­ã¿è¾¼ã¿
        try:
            if hasattr(arc_file, 'getvalue'):
                # Streamlit UploadedFileã®å ´åˆ
                arc_content = arc_file.getvalue()
                arc_encoding = detect_encoding(arc_content)
                arc_df = pd.read_csv(io.BytesIO(arc_content), encoding=arc_encoding)
            elif isinstance(arc_file, bytes):
                # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
                arc_encoding = detect_encoding(arc_file)
                arc_df = pd.read_csv(io.BytesIO(arc_file), encoding=arc_encoding)
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ
                arc_encoding = detect_encoding(arc_file)
                arc_df = pd.read_csv(arc_file, encoding=arc_encoding)
            
            if HAS_STREAMLIT:
                st.success(f"âœ… ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVèª­ã¿è¾¼ã¿å®Œäº†: {len(arc_df):,}è¡Œ")
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šã‚«ãƒ©ãƒ åã‚’è¡¨ç¤º
                with st.expander("ğŸ” ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVã‚«ãƒ©ãƒ ä¸€è¦§"):
                    st.write(f"ã‚«ãƒ©ãƒ æ•°: {len(arc_df.columns)}")
                    st.write(f"ã‚«ãƒ©ãƒ å: {list(arc_df.columns)}")
        except Exception as e:
            if HAS_STREAMLIT:
                st.error(f"âŒ ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            return None
        
        # ContractListèª­ã¿è¾¼ã¿
        try:
            if hasattr(contract_file, 'getvalue'):
                # Streamlit UploadedFileã®å ´åˆ
                contract_content = contract_file.getvalue()
                contract_encoding = detect_encoding(contract_content)
                contract_df = pd.read_csv(io.BytesIO(contract_content), encoding=contract_encoding)
            elif isinstance(contract_file, bytes):
                # ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
                contract_encoding = detect_encoding(contract_file)
                contract_df = pd.read_csv(io.BytesIO(contract_file), encoding=contract_encoding)
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ
                contract_encoding = detect_encoding(contract_file)
                contract_df = pd.read_csv(contract_file, encoding=contract_encoding)
            
            if HAS_STREAMLIT:
                st.success(f"âœ… ContractListèª­ã¿è¾¼ã¿å®Œäº†: {len(contract_df):,}è¡Œ")
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šã‚«ãƒ©ãƒ åã‚’è¡¨ç¤º
                with st.expander("ğŸ” ContractListã‚«ãƒ©ãƒ ä¸€è¦§"):
                    st.write(f"ã‚«ãƒ©ãƒ æ•°: {len(contract_df.columns)}")
                    st.write(f"ã‚«ãƒ©ãƒ å: {list(contract_df.columns)}")
        except Exception as e:
            if HAS_STREAMLIT:
                st.error(f"âŒ ContractListãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            return None
        
        # Phase 2: ã‚«ãƒ©ãƒ ç¢ºèª
        if HAS_STREAMLIT:
            st.info("ğŸ” Phase 2: å¿…é ˆã‚«ãƒ©ãƒ ç¢ºèª")
        
        # ã‚¢ãƒ¼ã‚¯æ®‹å‚µã®å¿…é ˆã‚«ãƒ©ãƒ 
        ark_required = {
            'contract_number': 'å¥‘ç´„ç•ªå·',
            'amount': 'ç®¡ç†å‰æ»ç´é¡'
        }
        
        # ContractListã®å¿…é ˆã‚«ãƒ©ãƒ 
        contract_required = {
            'takeover_number': 'å¼•ç¶™ç•ªå·',
            'management_number': 'ç®¡ç†ç•ªå·'
        }
        
        # ã‚«ãƒ©ãƒ å­˜åœ¨ç¢ºèª
        missing_ark = [col for col in ark_required.values() if col not in arc_df.columns]
        missing_contract = [col for col in contract_required.values() if col not in contract_df.columns]
        
        if missing_ark:
            if HAS_STREAMLIT:
                st.error(f"âŒ ã‚¢ãƒ¼ã‚¯æ®‹å‚µCSVã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_ark}")
                st.error(f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {list(arc_df.columns)}")
            return None
            
        if missing_contract:
            if HAS_STREAMLIT:
                st.error(f"âŒ ContractListã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_contract}")
                st.error(f"åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ : {list(contract_df.columns)}")
            return None
        
        if HAS_STREAMLIT:
            st.success("âœ… å¿…é ˆã‚«ãƒ©ãƒ ç¢ºèªå®Œäº†")
        
        # Phase 3: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ç´ä»˜ã‘
        if HAS_STREAMLIT:
            st.info("ğŸ”— Phase 3: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ç´ä»˜ã‘")
        
        # ã‚­ãƒ¼é …ç›®ã®æ­£è¦åŒ–
        arc_df = normalize_key_column(arc_df, 'å¥‘ç´„ç•ªå·')
        contract_df = normalize_key_column(contract_df, 'å¼•ç¶™ç•ªå·')
        
        # ãƒ‡ãƒ¼ã‚¿çµåˆï¼ˆå¥‘ç´„ç•ªå·ã¨å¼•ç¶™ç•ªå·ã§ç´ä»˜ã‘ï¼‰
        merged_df = pd.merge(
            arc_df[['å¥‘ç´„ç•ªå·', 'ç®¡ç†å‰æ»ç´é¡']],
            contract_df[['å¼•ç¶™ç•ªå·', 'ç®¡ç†ç•ªå·']],
            left_on='å¥‘ç´„ç•ªå·',
            right_on='å¼•ç¶™ç•ªå·',
            how='inner'
        )
        
        # ç´ä»˜ã‘çµ±è¨ˆ
        matched_count = len(merged_df)
        arc_total = len(arc_df)
        unmatch_count = arc_total - matched_count
        match_ratio = (matched_count / arc_total * 100) if arc_total > 0 else 0
        
        if HAS_STREAMLIT:
            st.write(f"ğŸ“Š ç´ä»˜ã‘çµæœ:")
            st.write(f"- ã‚¢ãƒ¼ã‚¯æ®‹å‚µç·æ•°: {arc_total:,}ä»¶")
            st.write(f"- ç´ä»˜ã‘æˆåŠŸ: {matched_count:,}ä»¶ ({match_ratio:.1f}%)")
            st.write(f"- ç´ä»˜ã‘å¤±æ•—: {unmatch_count:,}ä»¶")
        
        if matched_count == 0:
            if HAS_STREAMLIT:
                st.error("âŒ ç´ä»˜ã‘å‡¦ç†ã®çµæœã€å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã«ãªã‚Šã¾ã—ãŸ")
                st.error("åŸå› : å¥‘ç´„ç•ªå·ã¨å¼•ç¶™ç•ªå·ã®å€¤ãŒä¸€è‡´ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            return None
        
        # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆç®¡ç†ç•ªå·ã€ç®¡ç†å‰æ»ç´é¡ã®2åˆ—ã®ã¿ï¼‰
        output_df = merged_df[['ç®¡ç†ç•ªå·', 'ç®¡ç†å‰æ»ç´é¡']].copy()
        
        # ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
        output_df['ç®¡ç†ç•ªå·'] = output_df['ç®¡ç†ç•ªå·'].astype(str)
        output_df['ç®¡ç†å‰æ»ç´é¡'] = pd.to_numeric(output_df['ç®¡ç†å‰æ»ç´é¡'], errors='coerce').fillna(0).astype(int)
        
        # é‡è¤‡é™¤å»ï¼ˆç®¡ç†ç•ªå·ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€ç®¡ç†å‰æ»ç´é¡ã¯åˆè¨ˆï¼‰
        output_df = output_df.groupby('ç®¡ç†ç•ªå·', as_index=False).agg({
            'ç®¡ç†å‰æ»ç´é¡': 'sum'
        })
        
        # ã‚½ãƒ¼ãƒˆ
        output_df = output_df.sort_values('ç®¡ç†ç•ªå·')
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        current_date = datetime.now()
        output_filename = f"{current_date.strftime('%m%d')}ã‚¢ãƒ¼ã‚¯æ®‹å‚µ.csv"
        
        # å‡¦ç†ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        if HAS_STREAMLIT:
            st.success("âœ… Phase 4: å‡¦ç†å®Œäº†")
            st.write(f"ğŸ“Š æœ€çµ‚å‡ºåŠ›:")
            st.write(f"- ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(output_df):,}ä»¶")
            st.write(f"- ç®¡ç†å‰æ»ç´é¡åˆè¨ˆ: Â¥{output_df['ç®¡ç†å‰æ»ç´é¡'].sum():,}")
            st.write(f"- å¹³å‡æ»ç´é¡: Â¥{output_df['ç®¡ç†å‰æ»ç´é¡'].mean():,.0f}")
        
        return output_df, output_filename
        
    except Exception as e:
        if HAS_STREAMLIT:
            st.error(f"âŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            st.error("ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:")
            st.exception(e)
        return None